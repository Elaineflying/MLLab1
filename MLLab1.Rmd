---
title: "Machine Learning Lab1"
author: "Lepeng Zhang, Xuan Wang, Priyarani Patil"
date: "2023-11-08"
output: 
  pdf_document:
    latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Assignment 1. Handwritten digit recognition with Knearest neighbors.

*1.Import the data into R and divide it into training, validation and test sets (50%/25%/25%) by using the partitioning principle specified in the lecture slides.*

\textcolor{red}{Answer:}

```{r, knearest1, echo = FALSE}

# import data set
optdigits_data <- read.table('optdigits.csv', sep=",", header = 1)
head(optdigits_data, 5)

n=dim(optdigits_data)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.5))
train_data=optdigits_data[id,]
id1=setdiff(1:n, id)
id2=sample(id1, floor(n*0.25))
valid_data=optdigits_data[id2,]
id3=setdiff(id1,id2)
test_data=optdigits_data[id3,]

```

*2.Use training data to fit 30-nearest neighbor classifier with function kknn() and kernel=”rectangular” from package kknn and estimate • Confusion matrices for the training and test data (use table()) • Misclassification errors for the training and test data Comment on the quality of predictions for different digits and on the overall prediction quality.*

\textcolor{red}{Answer:}

```{r, knearest2, echo = FALSE}

# training data to fit 30-nearest neighbor classifier


```



*3.Find any 2 cases of digit “8” in the training data which were easiest to classify and 3 cases that were hardest to classify (i.e. having highest and lowest probabilities of the correct class). Reshape features for each of these cases as matrix 8x8 and visualize the corresponding digits (by using e.g. heatmap() function with parameters Colv=NA and Rowv=NA) and comment on whether these cases seem to be hard or easy to recognize visually.*


*4.Fit a K-nearest neighbor classifiers to the training data for different values of 𝐾𝐾 = 1,2, … , 30 and plot the dependence of the training and validation misclassification errors on the value of K (in the same plot). How does the model complexity change when K increases and how does it affect the training and validation errors? Report the optimal 𝐾𝐾 according to this plot. Finally, estimate the test error for the model having the optimal K, compare it with the training and validation errors and make necessary conclusions about the model quality.*





*5.Fit K-nearest neighbor classifiers to the training data for different values of 𝐾𝐾 = 1,2, … , 30, compute the error for the validation data as cross-entropy ( when computing log of probabilities add a small constant within log, e.g. 1e-15, to avoid numerical problems) and plot the dependence of the validation error on the value of 𝐾𝐾. What is the optimal 𝐾𝐾 value here? Assuming that response has multinomial distribution, why might the cross-entropy be a more suitable choice of the error function than the misclassification error for this problem?*




# Assignment 2. Linear regression and ridge regression


# Assignment 3. Logistic regression and basis function expansion


# Appendix: 

knearest.R

```{r ref.label=c('knearest1', 'knearest2', 'knearest3', 'knearest4'), echo=TRUE, eval=FALSE}

```

