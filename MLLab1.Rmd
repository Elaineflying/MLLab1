---
title: "Machine Learning Lab1"
author: "Lepeng Zhang, Xuan Wang, Priyarani Patil"
date: "2023-11-08"
output: 
  pdf_document:
    latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Assignment 1. Handwritten digit recognition with Knearest neighbors.

*1.*

\textcolor{red}{Answer:}

```{r, knearest1, echo = FALSE}
# load necessary libraries
library(ggplot2)
library(kknn)

# import data set
optdigits_data <- read.csv('optdigits.csv', header = FALSE)
colnames(optdigits_data) <- c(paste0("a",1:64),"digit")
optdigits_data$digit <- as.factor(optdigits_data$digit)
#head(optdigits_data, 5)

n=dim(optdigits_data)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.5))
train_data=optdigits_data[id,]
id1=setdiff(1:n, id)
id2=sample(id1, floor(n*0.25))
valid_data=optdigits_data[id2,]
id3=setdiff(id1,id2)
test_data=optdigits_data[id3,]

```

*2.*

\textcolor{red}{Answer:}

```{r, knearest2, echo = FALSE}
# 30-nearest neighbor classification
k_fit_train <- kknn(formula = digit ~ ., train_data, train_data, k = 30, kernel = "rectangular")
k_fit_test <- kknn(formula = digit ~ ., train_data, test_data, k = 30, kernel = "rectangular")

# Confusion matrices for the training and test data
train_confusion <- table(train_data$digit, fitted(k_fit_train))
cat("Confusion Matrix for Training Data:\n")
print(train_confusion)

test_confusion <- table(test_data$digit, fitted(k_fit_test))
cat("Confusion Matrix for Test Data:\n")
print(test_confusion)

# Misclassification errors for the training and test data
train_error <- 1 - sum(diag(train_confusion)) / sum(train_confusion)
cat("Misclassification errors for the training data are: ", train_error, "\n" )

test_error <- 1 - sum(diag(test_confusion)) / sum(test_confusion)
cat("Misclassification errors for the test data are: ", test_error, "\n" )


# the quality of predictions for different digits
for ( i in 1:nrow(test_confusion)) {
  digit_accuracy <- test_confusion[i,i] / sum(test_confusion[i,])
  cat("The accuracy of prediction of for digit ", i-1, " is: ", digit_accuracy, "\n")
}

overall_accuracy <- sum(diag(test_confusion)) / sum(test_confusion)
cat("The overall accuracy of prediction is:", overall_accuracy, "\n")

```

*3.*

\textcolor{red}{Answer:}

```{r, knearest3, echo = FALSE}
# Get probabilities of class "8"
probabilities <- k_fit_train$prob[,"8"]

# Get indices of training data for class "8"
indices_8 <- which(train_data$digit == "8")

# Get probabilities for class "8"
probabilities_8 <- probabilities[indices_8]

# Find 2 easiest (highest probability) and 3 hardest (lowest probability) to classify cases
easiest_indices <- indices_8[order(probabilities_8, decreasing = TRUE)[1:2]]
hardest_indices <- indices_8[order(probabilities_8)[1:3]]

# Reshape features as 8x8 matrix and visualize
for (index in c(easiest_indices, hardest_indices)) {
  digit_8 <- matrix(as.numeric(train_data[index, 1:64]), nrow = 8, byrow = TRUE)
  heatmap(digit_8, Colv = NA, Rowv = NA, main = paste("Digit '8', Index:", index))
}

```

*4.*

\textcolor{red}{Answer:}

```{r, knearest4, echo = FALSE}
# Fit KNN for different K values and plot errors
errors <- data.frame()
for (k in 1:30) {
  fit <- kknn(digit ~ ., train_data, valid_data, k = k, kernel = "rectangular")
  pred <- fit$fitted.values
  confusion_matrix <- table(valid_data$digit, pred)
  error <- 1 - sum(diag(confusion_matrix)) / sum(confusion_matrix)
  errors <- rbind(errors, data.frame(K = k, Error = error))
}
# Plot the misclassification errors on the value of K
plot(errors$K, errors$Error, type = "b", col='orange', main="Misclassification errors on the value of K", xlab = "K value", ylab = "error")

```

*5.*

\textcolor{red}{Answer:}

```{r, knearest5, echo = FALSE}
# Initialize a data frame to store the results
ce_errors <- data.frame(k_value = integer(), cross_entropy_error = numeric())

for (k in 1:30) {
  # Fit K-nearest neighbor classifier
  fit <- kknn(digit ~ ., train_data, valid_data, k = k, kernel = "rectangular")

  # Get predicted probabilities
  predicted_probabilities <- fit$prob

  # Compute cross-entropy error
  #actual_probabilities <- ifelse(valid_data$digit == "8", 1, 0)
  #cross_entropy <- -sum(actual_probabilities * log(predicted_probabilities + 1e-15))
  cross_entropy <- -sum(as.numeric(valid_data$digit) * log(predicted_probabilities + 1e-15))

  ce_errors <- rbind(ce_errors, data.frame(k_value = k, cross_entropy_error = cross_entropy))
}

# Plot the results
plot(ce_errors$k_value, ce_errors$cross_entropy_error, type = "b", col='orange', main="Cross Entropy errors on the value of K", xlab = "K value", ylab = "Cross Entropy error")

# Find the optimal K
optimal_k <- ce_errors$k_value[which.min(ce_errors$cross_entropy_error)]
cat("The optimal k is:", optimal_k, "\n")
```


# Assignment 2. Linear regression and ridge regression
*1.*

\textcolor{red}{Answer:}

```{r, linear_ridge1, echo = FALSE}
# import data set
parkinson_data <- read.csv('parkinsons.csv', header = TRUE)
#head(parkinson_data, 5)

# Split the data into training and test sets
parkinson_data <- subset(parkinson_data, select = -c(1:4,6))
n=dim(parkinson_data)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.6))
train_data=parkinson_data[id,]
test_data=parkinson_data[-id,]

# Scale the data
#scaled_train_data <- as.data.frame(cbind(scale(subset(train_data, select = -c(motor_UPDRS))), motor_UPDRS=train_data$motor_UPDRS))
#scaled_test_data <- as.data.frame(cbind(scale(subset(test_data, select = -c(motor_UPDRS))), motor_UPDRS=test_data$motor_UPDRS))
scaled_train_data <- as.data.frame(scale(train_data))
scaled_test_data <- as.data.frame(scale(test_data))

```


*2.*

\textcolor{red}{Answer:}

```{r, linear_ridge2, echo = FALSE}
# linear regression model
lm_model <- lm(motor_UPDRS ~ ., data = scaled_train_data)

# Training and test MSE
train_pred <- predict(lm_model, newdata = scaled_train_data)
train_mse <- mean((scaled_train_data$motor_UPDRS - train_pred)^2)
cat("The training data's MSE is: ", train_mse)

test_pred <- predict(lm_model, newdata = scaled_test_data)
test_mse <- mean((scaled_test_data$motor_UPDRS - test_pred)^2)
cat("The training data's MSE is: ", test_mse)

# Significant variables
summary(lm_model)

```


*3.*

\textcolor{red}{Answer:}

```{r, linear_ridge3, echo = FALSE}
# Log-likelihood function
log_likelihood_fun <- function(theta, sigma, y, X) {
  n <- length(y)
  log_likelihood <- -n/2 * log(2*pi*sigma^2) - 1/(2*sigma^2) * sum((y - X%*%theta)^2)
  return(log_likelihood)
}

# Ridge log-likelihood function
ridge_log_Likelihood_fun <- function(theta, sigma, lambda, y, X) {
  log_like <- log_likelihood_fun(theta, sigma, y, X)
  ridge_penalty <- lambda * sum(theta^2)
  return(-log_like + ridge_penalty)
}

# Ridge log-likelihood optimization function
ridge_log_likelihood_opt <- function(lambda, y, X) {
  start <- c(rep(0, ncol(X)), sd(y)) # combine theta and sigma into one numeric vector
  theta_length <- ncol(X)

  # Define a function for the optim() call
  fn_to_optim <- function(params) {
    theta <- params[1:theta_length]
    sigma <- params[theta_length + 1]
    return(-ridge_log_Likelihood_fun(theta, sigma, lambda, y, X))
  }

  # Call optim()
  opt_res <- optim(start, fn = fn_to_optim, method = "BFGS")

  # Return theta and sigma separately
  return(list(theta = opt_res$par[1:theta_length], sigma = opt_res$par[theta_length + 1]))
}

# Degrees of freedom function
df_fun <- function(lambda, X) {
  H <- solve(t(X) %*% X + lambda * diag(ncol(X))) %*% t(X) %*% X
  return(sum(diag(H)))
}

y <- scaled_train_data$motor_UPDRS
X <- as.matrix(subset(scaled_train_data, select = -c(motor_UPDRS)))
# Ridge optimization for different lambdas
lambdas <- c(1, 100, 1000)
opt_params <- lapply(lambdas, function(lambda) ridge_log_likelihood_opt(lambda, y, X))


```



*4.*

\textcolor{red}{Answer:}

```{r, linear_ridge4, echo = FALSE}
# Predictions and MSE for different lambdas
ridge_train_preds <- lapply(1:length(opt_params), function(i) {
  theta <- opt_params[[i]]$theta
  as.matrix(subset(scaled_train_data, select = -c(motor_UPDRS))) %*% theta
})

#train_mses <- colMeans((scaled_train_data$motor_UPDRS - ridge_train_preds)^2)
ridge_train_mse <- sapply(ridge_train_preds, function(pred) mean((pred - scaled_train_data$motor_UPDRS)^2))
cat("Ridge Training MSE:", ridge_train_mse, "\n")

ridge_test_preds <- lapply(1:length(opt_params), function(i) {
  theta <- opt_params[[i]]$theta
  as.matrix(subset(scaled_test_data, select = -c(motor_UPDRS))) %*% theta
})

ridge_test_mse <- sapply(ridge_test_preds, function(pred) mean((pred - scaled_test_data$motor_UPDRS)^2))
cat("Ridge Test MSE:", ridge_test_mse, "\n")

# Degrees of freedom for different lambdas
degrees_of_freedom <- sapply(lambdas, function(lambda) df_fun(lambda, X))
cat("Degrees of Freedom:", degrees_of_freedom, "\n")

```



# Assignment 3. Logistic regression and basis function expansion

*1.*


\textcolor{red}{Answer:}

```{r, logistic1, echo = FALSE}
library(ggplot2)
library(caret)

# import data set
pima_data <- read.csv('pima-indians-diabetes.csv', header = FALSE)
colnames(pima_data) <- c("num_of_pregnant", "plasma_glucose_concentration", "blood_pressure",
                                          "skinfold_thickness", "serum_insulin", "bmi",
                                          "diabetes_predigree", "age",  "diabetes")
#head(pima_data, 5)


# Scatterplot
ggplot(pima_data, aes(x=plasma_glucose_concentration, y=age, color=as.factor(diabetes))) +
  geom_point() +
  labs(color="diabetes", title='Scatter Plot of Plasma Glucose Concentration on Age')

```


*2.*

\textcolor{red}{Answer:}

```{r, logistic2, echo = FALSE}
# Logistic Regression
logsitic_model <- glm(diabetes ~ plasma_glucose_concentration + age, data=pima_data, family=binomial)
summary(logsitic_model)

# Predict probabilities
probabilities <- predict(logsitic_model, type="response")
print(paste0("Probability(Diabetes=1) = 1 / (1 + exp(-(",coef(logsitic_model)[1]," + ", coef(logsitic_model)[2], "*x1 + ", coef(logsitic_model)[3], "*x2)))"))

# Classify observations
predictions <- ifelse(probabilities >= 0.5, 1, 0)

# Compute misclassification error
mis_error <- mean(predictions != pima_data$diabetes)
cat("Misclassification Error:", mis_error, "\n")

# Scatter plot with predicted values
ggplot(data.frame(pima_data, predictions), aes(x=plasma_glucose_concentration, y=age, color=as.factor(predictions))) +
  geom_point() +
  labs(color="predictions",title='Scatter Plot with Predicted Diabetes Values')

```


*3.*

\textcolor{red}{Answer:}

```{r, logistic3, echo = FALSE}
# Decision boundary equation
# Decision boundary is where the logistic function equals 0.5
# 0 = intercept + coef1*x1 + coef2*x2
# x2 = -(intercept + coef1*x1) / coef2

#decision_boundary <- -(coef(logsitic_model)[1] + coef(logsitic_model)[2]*pima_data$age) / coef(logsitic_model)[3]
decision_boundary_x1 <- pima_data$plasma_glucose_concentration
decision_boundary_x2 <- -(coef(logsitic_model)[1] + coef(logsitic_model)[2]*decision_boundary_x1) / coef(logsitic_model)[3]

# Add decision boundary to scatter plot
ggplot(data.frame(pima_data, predictions), aes(x=plasma_glucose_concentration, y=age, color=as.factor(predictions))) +
  geom_point() +
  geom_line(aes(x=decision_boundary_x1, y=decision_boundary_x2), color='red', linetype='dashed', linewidth=1) + xlim(0, 200) + ylim(20, 80) +
  labs(color="diabetes", title='Scatter Plot of Plasma Glucose Concentration on Age with Decision Boundary')



```


*4.*

\textcolor{red}{Answer:}

```{r, logistic4, echo = FALSE}
# Predictions with different thresholds
thresholds <- c(0.2, 0.8)

for ( threshold in thresholds) {
  pred_threshold <- ifelse(predict(logsitic_model, newdata=pima_data, type='response') >= threshold, 1, 0)

  # Scatter plot with predicted values and threshold
  scatter_plot <- ggplot(data.frame(pima_data, pred_threshold), aes(x=plasma_glucose_concentration, y=age, color=as.factor(pred_threshold))) +
    geom_point() +
    labs(color="pred_threshold", title=paste('Scatter Plot with Predicted Diabetes Values (Threshold =', threshold, ')'))
  print(scatter_plot)
}

```

*5.*

\textcolor{red}{Answer:}

```{r, logistic5, echo = FALSE}


# Create new features
pima_data$z1 <- pima_data$plasma_glucose_concentration^4
pima_data$z2 <- pima_data$plasma_glucose_concentration^3 * pima_data$age
pima_data$z3 <- pima_data$plasma_glucose_concentration^2 * pima_data$age^2
pima_data$z4 <- pima_data$plasma_glucose_concentration * pima_data$age^3
pima_data$z5 <- pima_data$age^4

# Logistic Regression with new features
model_expanded <- glm(diabetes ~ plasma_glucose_concentration + age + z1 + z2 + z3 + z4 + z5, data=pima_data, family=binomial)

# Predict probabilities and classify observations
probabilities_expanded <- predict(model_expanded, type="response")
predictions_expanded <- ifelse(probabilities_expanded >= 0.5, 1, 0)

# Scatter plot for the model with expanded features
ggplot(data.frame(pima_data, predictions_expanded), aes(x=plasma_glucose_concentration, y=age, color=as.factor(predictions_expanded))) +
  geom_point() +
  labs(color="predictions_expanded",title='Scatter Plot with Predicted Diabetes Values (Expanded Features)')

# Compute misclassification error
mis_error_expanded <- mean(predictions_expanded != pima_data$diabetes)
cat("Misclassification Error (Expanded Features):", mis_error_expanded, "\n")

```



# Appendix: 

knearest.R

```{r ref.label=c('knearest1', 'knearest2', 'knearest3', 'knearest4', 'knearest5'), echo=TRUE, eval=FALSE}

```


linear_ridge.R

```{r ref.label=c('linear_ridge1', 'linear_ridge2', 'linear_ridge3', 'linear_ridge4'), echo=TRUE, eval=FALSE}

```


logistic.R

```{r ref.label=c('logistic1', 'logistic2', 'logistic3', 'logistic4', 'logistic5'), echo=TRUE, eval=FALSE}

```

